{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y9Hqo6UZMsg2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import threading\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "def getData(url, dirname=\"data\", img_shape=(100,100)):\n",
        "    data = pd.read_csv(url, sep=\"\\t\", skiprows=2, header=None, names=['Name', 'imagenum', 'url', 'rect', 'md5'])\n",
        "    total_images = data.shape[0]  # Total number of images fetched from the URL\n",
        "    print(f\"Total number of images fetched from {url}: {total_images}\")\n",
        "\n",
        "    total_personalities = data.Name.nunique()\n",
        "    current = 0\n",
        "    if not os.path.exists(dirname):\n",
        "        os.mkdir(dirname)\n",
        "\n",
        "    j = 0\n",
        "    # Limit the loop to iterate over the first 100 rows of the DataFrame\n",
        "    for i in range(min(100, data.shape[0])):\n",
        "        if not os.path.exists(os.path.join(dirname, data.iloc[i].Name)):\n",
        "            os.mkdir(os.path.join(dirname, data.iloc[i].Name))\n",
        "            current += 1\n",
        "            print(\"{} : {}/{} {:.2f}% done\".format(dirname, current, total_personalities, i * 100 / 100))\n",
        "            j = 0\n",
        "        try:\n",
        "            resp = urlopen(data.iloc[i].url, timeout=1)\n",
        "            image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "            image = cv2.imdecode(image, cv2.COLOR_BGR2GRAY)\n",
        "            p1, p2, p3, p4 = tuple(map(int, data.iloc[i].rect.split(',')))\n",
        "            image = image[p2:p4, p1:p3]\n",
        "            image = cv2.resize(image, img_shape, interpolation=cv2.INTER_AREA)\n",
        "            plt.imsave(os.path.join(dirname, data.iloc[i].Name, str(j) + '.jpg'), image)\n",
        "            j += 1\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDDa0Y_vMvPN",
        "outputId": "fc5609a2-1dc2-48c8-a70e-80b2acf02d81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval : 3/60 1.67% done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_e = threading.Thread(target = getData,\n",
        "                           args = ('http://www.cs.columbia.edu/CAVE/databases/pubfig/download/dev_urls.txt', 'eval'))\n",
        "data_e.start()"
      ],
      "metadata": {
        "id": "4UvbxOPvMvSO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_d = threading.Thread(target = getData,\n",
        "                           args = ('http://www.cs.columbia.edu/CAVE/databases/pubfig/download/eval_urls.txt', 'train'))\n",
        "data_d.start()"
      ],
      "metadata": {
        "id": "nrx4NF84MvUm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_d.join()\n",
        "data_e.join()"
      ],
      "metadata": {
        "id": "DhK_k8cAMvXd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getMiniBatch(batch_size=32,prob=0.5,path = \"train\"):\n",
        "    persons = os.listdir(path)\n",
        "    left = [];right = []\n",
        "    target = []\n",
        "    for _ in range(batch_size):\n",
        "        res = np.random.choice([0,1],p=[1-prob,prob])\n",
        "        if res==0:\n",
        "            p1,p2 = tuple(np.random.choice(persons,size=2,replace=False))\n",
        "            while len(os.listdir(os.path.join(path,p1)))<1 or len(os.listdir(os.path.join(path,p2)))<1:\n",
        "                p1,p2 = tuple(np.random.choice(persons,size=2,replace=False))\n",
        "            p1 = os.path.join(path,p1,random.choice(os.listdir(os.path.join(path,p1))))\n",
        "            p2 = os.path.join(path,p2,random.choice(os.listdir(os.path.join(path,p2))))\n",
        "            p1,p2 = np.expand_dims(cv2.imread(p1,0),-1),np.expand_dims(cv2.imread(p2,0),-1)\n",
        "            left.append(p1);right.append(p2)\n",
        "            target.append(0)\n",
        "        else:\n",
        "            p = np.random.choice(persons)\n",
        "            while len(os.listdir(os.path.join(path,p)))<2:\n",
        "                p = np.random.choice(persons)\n",
        "            p1,p2 = tuple(np.random.choice( os.listdir(os.path.join(path,p)), size=2, replace=False ))\n",
        "            p1,p2 = os.path.join(path,p,p1),os.path.join(path,p,p2)\n",
        "            p1,p2 = np.expand_dims(cv2.imread(p1,0),-1),np.expand_dims(cv2.imread(p2,0),-1)\n",
        "            left.append(p1);right.append(p2)\n",
        "            target.append(1)\n",
        "    return [np.array(left),np.array(right)],np.array(target)"
      ],
      "metadata": {
        "id": "qUsWf49BMvaC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_oneshot(model,N,verbose=0):\n",
        "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "    if verbose:\n",
        "        pass\n",
        "        #print(\"Evaluating model on {} one-shot learning tasks ...\".format(N))\n",
        "    inputs, targets = getMiniBatch(N,path=\"eval\")\n",
        "    probs = model.predict(inputs)\n",
        "    output = (np.squeeze(probs)>0.5)*1\n",
        "    percent_correct = (output==targets).sum()*100/N\n",
        "    if verbose:\n",
        "        print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
        "    return percent_correct"
      ],
      "metadata": {
        "id": "MuTYNs8cNigZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Conv2D, Dense, Flatten,MaxPooling2D\n",
        "from keras.layers import Lambda, Subtract\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "def W_init(shape, dtype=None, name=None):\n",
        "    \"\"\"Initialize weights as in paper\"\"\"\n",
        "    values = np.random.normal(loc=0, scale=1e-2, size=shape)\n",
        "    return K.variable(values, dtype=dtype, name=name)\n",
        "\n",
        "#//TODO: figure out how to initialize layer biases in keras.\n",
        "def b_init(shape, dtype=None, name=None):\n",
        "    \"\"\"Initialize bias as in paper\"\"\"\n",
        "    values = np.random.normal(loc=0.5, scale=1e-2, size=shape)\n",
        "    return K.variable(values, dtype=dtype, name=name)\n",
        "\n",
        "input_shape = (100, 100, 1)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "\n",
        "#build convnet to use in each siamese 'leg'\n",
        "convnet = Sequential()\n",
        "convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
        "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(128,(7,7),activation='relu',\n",
        "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
        "convnet.add(Flatten())\n",
        "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "\n",
        "#encode each of the two inputs into a vector with the convnet\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "\n",
        "#merge two encoded inputs with the l1 distance between them\n",
        "subtracted = Subtract()( [encoded_l,encoded_r]  )\n",
        "both = Lambda(lambda x: abs(x))(subtracted)\n",
        "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
        "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "\n",
        "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
        "\n",
        "optimizer = Adam(0.00006)\n",
        "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
        "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
        "\n",
        "siamese_net.count_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMKnPkMVNii-",
        "outputId": "febe52c7-d36c-4546-de67-561a24592b92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27417409"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_every = 7000\n",
        "loss_every = 500\n",
        "batch_size = 32\n",
        "N = 1000\n",
        "best = 0\n",
        "loss_history = []\n",
        "for i in range(0,900000):\n",
        "    (inputs,targets)= getMiniBatch(batch_size,path=\"train\")\n",
        "    loss=siamese_net.train_on_batch(inputs,targets)\n",
        "    loss_history.append(loss)\n",
        "    if i % loss_every == 0:\n",
        "        vloss = siamese_net.test_on_batch(*getMiniBatch(batch_size,path=\"eval\"))\n",
        "        print(\"iteration {}, training loss: {:.7f}, validation loss : {:.7f}\".format(i,np.mean(loss_history),vloss))\n",
        "        loss_history.clear()\n",
        "        val_acc = test_oneshot(siamese_net,N,verbose=True)\n",
        "        if val_acc >= best:\n",
        "            print(\"saving\")\n",
        "            siamese_net.save('saved_best')\n",
        "            best=val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKlkwJnuNilT",
        "outputId": "44737a92-7c0a-4f84-8a7c-e6ac3e2d01ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval : 7/60 6.81% done\n",
            "train : 6/140 3.23% done\n",
            "iteration 0, training loss: 3.3581634, validation loss : 3.3804257\n",
            "31/32 [============================>.] - ETA: 1seval : 8/60 7.79% done\n",
            "32/32 [==============================] - 62s 2s/step\n",
            "Got an average of 48.8% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 7/140 4.16% done\n",
            "train : 8/140 4.56% done\n",
            "eval : 9/60 10.79% done\n",
            "eval : 10/60 11.50% done\n",
            "eval : 11/60 13.41% done\n",
            "train : 9/140 7.07% done\n",
            "eval : 12/60 15.00% done\n",
            "train : 10/140 7.82% done\n",
            "eval : 13/60 16.72% done\n",
            "train : 11/140 8.30% done\n",
            "eval : 14/60 19.19% done\n",
            "train : 12/140 8.84% done\n",
            "eval : 15/60 19.88% done\n",
            "train : 13/140 9.56% done\n",
            "eval : 16/60 22.36% done\n",
            "train : 14/140 11.37% done\n",
            "eval : 17/60 24.69% done\n",
            "eval : 18/60 25.18% done\n",
            "eval : 19/60 25.73% done\n",
            "train : 15/140 12.09% done\n",
            "eval : 20/60 27.14% done\n",
            "train : 16/140 13.13% done\n",
            "eval : 21/60 28.28% done\n",
            "train : 17/140 13.47% done\n",
            "train : 18/140 13.73% done\n",
            "eval : 22/60 29.03% done\n",
            "train : 19/140 13.89% done\n",
            "eval : 23/60 31.13% done\n",
            "eval : 24/60 33.25% done\n",
            "train : 20/140 16.37% done\n",
            "eval : 25/60 33.92% done\n",
            "train : 21/140 16.88% done\n",
            "eval : 26/60 35.51% done\n",
            "train : 22/140 17.52% done\n",
            "eval : 27/60 37.95% done\n",
            "train : 23/140 18.65% done\n",
            "eval : 28/60 39.34% done\n",
            "train : 24/140 19.02% done\n",
            "eval : 29/60 40.51% done\n",
            "train : 25/140 19.22% done\n",
            "eval : 30/60 43.51% done\n",
            "train : 26/140 20.19% done\n",
            "train : 27/140 20.74% done\n",
            "eval : 31/60 44.77% done\n",
            "train : 28/140 21.98% done\n",
            "eval : 32/60 46.81% done\n",
            "train : 29/140 22.16% done\n",
            "train : 30/140 22.45% done\n",
            "eval : 33/60 48.04% done\n",
            "train : 31/140 22.87% done\n",
            "eval : 34/60 49.06% done\n",
            "train : 32/140 23.30% done\n",
            "eval : 35/60 51.43% done\n",
            "train : 33/140 24.29% done\n",
            "eval : 36/60 53.42% done\n",
            "eval : 37/60 53.89% done\n",
            "train : 34/140 25.11% done\n",
            "train : 35/140 25.30% done\n",
            "eval : 38/60 55.70% done\n",
            "train : 36/140 27.36% done\n",
            "train : 37/140 27.51% done\n",
            "eval : 39/60 67.30% done\n",
            "eval : 40/60 68.14% done\n",
            "train : 38/140 28.37% done\n",
            "train : 39/140 28.91% done\n",
            "eval : 41/60 70.18% done\n",
            "eval : 42/60 71.58% done\n",
            "train : 40/140 29.49% done\n",
            "eval : 43/60 72.84% done\n",
            "train : 41/140 30.53% done\n",
            "eval : 44/60 73.97% done\n",
            "train : 42/140 31.04% done\n",
            "train : 43/140 31.31% done\n",
            "eval : 45/60 75.65% done\n",
            "eval : 46/60 76.51% done\n",
            "train : 44/140 31.85% done\n",
            "eval : 47/60 77.26% done\n",
            "train : 45/140 32.46% done\n",
            "eval : 48/60 80.67% done\n",
            "train : 46/140 33.56% done\n",
            "eval : 49/60 82.04% done\n",
            "train : 47/140 34.19% done\n",
            "eval : 50/60 84.08% done\n",
            "train : 48/140 34.41% done\n",
            "eval : 51/60 87.17% done\n",
            "eval : 52/60 88.71% done\n",
            "train : 49/140 35.68% done\n",
            "eval : 53/60 89.31% done\n",
            "train : 50/140 36.06% done\n",
            "iteration 500, training loss: 1.8636313, validation loss : 1.2229426\n",
            "32/32 [==============================] - 58s 2s/step\n",
            "Got an average of 62.3% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval : 54/60 91.20% done\n",
            "train : 51/140 36.69% done\n",
            "eval : 55/60 92.24% done\n",
            "train : 52/140 37.15% done\n",
            "eval : 56/60 92.87% done\n",
            "eval : 57/60 93.58% done\n",
            "train : 53/140 37.50% done\n",
            "eval : 58/60 95.24% done\n",
            "eval : 59/60 95.89% done\n",
            "train : 54/140 38.71% done\n",
            "eval : 60/60 98.76% done\n",
            "train : 55/140 39.86% done\n",
            "train : 56/140 40.66% done\n",
            "train : 57/140 40.83% done\n",
            "train : 58/140 41.61% done\n",
            "train : 59/140 42.10% done\n",
            "train : 60/140 42.80% done\n",
            "train : 61/140 43.25% done\n",
            "train : 62/140 43.69% done\n",
            "train : 63/140 44.06% done\n",
            "train : 64/140 44.51% done\n",
            "train : 65/140 44.68% done\n",
            "train : 66/140 46.12% done\n",
            "train : 67/140 47.25% done\n",
            "train : 68/140 48.12% done\n",
            "train : 69/140 48.48% done\n",
            "train : 70/140 48.90% done\n",
            "train : 71/140 50.22% done\n",
            "train : 72/140 51.50% done\n",
            "train : 73/140 51.87% done\n",
            "train : 74/140 52.49% done\n",
            "train : 75/140 53.35% done\n",
            "train : 76/140 53.72% done\n",
            "train : 77/140 53.99% done\n",
            "train : 78/140 54.86% done\n",
            "train : 79/140 55.26% done\n",
            "train : 80/140 55.97% done\n",
            "train : 81/140 56.78% done\n",
            "train : 82/140 57.31% done\n",
            "train : 83/140 58.03% done\n",
            "train : 84/140 59.35% done\n",
            "train : 85/140 59.79% done\n",
            "train : 86/140 61.34% done\n",
            "train : 87/140 61.76% done\n",
            "train : 88/140 65.30% done\n",
            "iteration 1000, training loss: 1.0229649, validation loss : 0.8788821\n",
            "18/32 [===============>..............] - ETA: 24strain : 89/140 65.99% done\n",
            "32/32 [==============================] - 56s 2s/step\n",
            "Got an average of 64.4% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 90/140 66.70% done\n",
            "train : 91/140 67.48% done\n",
            "train : 92/140 68.06% done\n",
            "train : 93/140 68.99% done\n",
            "train : 94/140 69.34% done\n",
            "train : 95/140 70.26% done\n",
            "train : 96/140 71.09% done\n",
            "train : 97/140 71.68% done\n",
            "train : 98/140 72.13% done\n",
            "train : 99/140 72.44% done\n",
            "train : 100/140 72.79% done\n",
            "train : 101/140 73.38% done\n",
            "train : 102/140 74.10% done\n",
            "train : 103/140 74.26% done\n",
            "train : 104/140 75.01% done\n",
            "train : 105/140 75.31% done\n",
            "train : 106/140 76.75% done\n",
            "train : 107/140 77.00% done\n",
            "train : 108/140 78.05% done\n",
            "train : 109/140 80.94% done\n",
            "train : 110/140 81.36% done\n",
            "train : 111/140 81.64% done\n",
            "train : 112/140 81.93% done\n",
            "train : 113/140 82.36% done\n",
            "train : 114/140 82.52% done\n",
            "train : 115/140 83.37% done\n",
            "train : 116/140 84.18% done\n",
            "train : 117/140 84.95% done\n",
            "train : 118/140 85.26% done\n",
            "train : 119/140 85.53% done\n",
            "train : 120/140 86.31% done\n",
            "train : 121/140 86.51% done\n",
            "train : 122/140 87.20% done\n",
            "train : 123/140 88.34% done\n",
            "train : 124/140 88.88% done\n",
            "train : 125/140 89.88% done\n",
            "train : 126/140 90.04% done\n",
            "train : 127/140 90.52% done\n",
            "train : 128/140 91.08% done\n",
            "train : 129/140 91.94% done\n",
            "train : 130/140 92.50% done\n",
            "train : 131/140 93.04% done\n",
            "train : 132/140 93.43% done\n",
            "train : 133/140 94.07% done\n",
            "iteration 1500, training loss: 0.8243145, validation loss : 0.7438051\n",
            "32/32 [==============================] - 55s 2s/step\n",
            "Got an average of 69.1% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 134/140 95.28% done\n",
            "train : 135/140 95.94% done\n",
            "train : 136/140 96.34% done\n",
            "train : 137/140 97.22% done\n",
            "train : 138/140 98.06% done\n",
            "train : 139/140 99.02% done\n",
            "train : 140/140 99.76% done\n",
            "iteration 2000, training loss: 0.7285835, validation loss : 0.7124769\n",
            "32/32 [==============================] - 59s 2s/step\n",
            "Got an average of 69.2% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 2500, training loss: 0.6701246, validation loss : 0.6403880\n",
            "32/32 [==============================] - 53s 2s/step\n",
            "Got an average of 69.2% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 3000, training loss: 0.6341374, validation loss : 0.5762537\n",
            "32/32 [==============================] - 56s 2s/step\n",
            "Got an average of 70.3% 1000 way one-shot learning accuracy\n",
            "saving\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/__init__.py:144: UserWarning: The `keras.initializers.serialize()` API should only be used for objects of type `keras.initializers.Initializer`. Found an instance of type <class 'function'>, which may lead to improper serialization.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc = None\n",
        "while val_acc==None:\n",
        "    try:\n",
        "        siamese_net.load_weights(\"saved_best\")\n",
        "        val_acc = test_oneshot(siamese_net,1000,verbose=True)\n",
        "        print(\"Accuracy: {}\".format(val_acc))\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "vu7wuvd7Ninr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#haarcascade_frontalface_default.xml is saved model for face detection\n",
        "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "def giveAllFaces(image,BGR_input=True,BGR_output=False):\n",
        "    \"\"\"\n",
        "      return GRAY cropped_face,x,y,w,h\n",
        "    \"\"\"\n",
        "    gray = image.copy()\n",
        "    if BGR_input:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = faceCascade.detectMultiScale(\n",
        "        gray,\n",
        "        scaleFactor=1.3,\n",
        "        minNeighbors=3,\n",
        "        minSize=(30, 30)\n",
        "    )\n",
        "    if BGR_output:\n",
        "        for (x, y, w, h) in faces:\n",
        "            yield image[y:y+h,x:x+w,:],x,y,w,h\n",
        "    else:\n",
        "        for (x, y, w, h) in faces:\n",
        "            yield gray[y:y+h,x:x+w],x,y,w,h\n",
        "\n",
        "#to draw rectangle\n",
        "#for (_,x, y, w, h) in giveAllFaces(image):\n",
        "#    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "import math\n",
        "def test(path=\"sample/tbbt.jpg\"):\n",
        "    image = cv2.imread(path)\n",
        "    faces= [ cv2.resize(face,(100,100),interpolation = cv2.INTER_AREA) for face,_,_,_,_ in giveAllFaces(image,BGR_output=True)]\n",
        "    print(\"Total Faces Detected: {}\".format(len(faces)))\n",
        "    t = math.ceil(len(faces)/2)\n",
        "    i,one = 0,[]\n",
        "    while i<t:\n",
        "        one.append(faces[i]);i+=1\n",
        "    two = one.copy()\n",
        "    while i<len(faces):\n",
        "        two[i-t] = faces[i];i+=1\n",
        "    plt.imshow(np.vstack([np.hstack(one),np.hstack(two)]))\n",
        "\n",
        "test() #other options - got.jpg, friends.jpg"
      ],
      "metadata": {
        "id": "m7JtVWVuNip-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def putBoxText(image,x,y,w,h,text=\"unknown\"):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    cv2.putText(image,text, (x,y-6), font, 1, (0, 255, 0), 2, cv2.LINE_AA)"
      ],
      "metadata": {
        "id": "9SABl2VdNiso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def putCharacters(image,db=\"database\"):\n",
        "    dbs = os.listdir(db)\n",
        "    right = np.array([ np.expand_dims(cv2.imread(os.path.join(db,x),0),-1) for x in dbs ])\n",
        "    names = [ os.path.splitext(x)[0] for x in dbs ]\n",
        "    for face,x,y,w,h in giveAllFaces(image):\n",
        "        face = cv2.resize(face,(100,100),interpolation = cv2.INTER_AREA)\n",
        "        face = np.expand_dims(face,-1)\n",
        "        left = np.array([face for _ in range(len(dbs))])\n",
        "        probs = np.squeeze(siamese_net.predict([left,right]))\n",
        "        index = np.argmax(probs)\n",
        "        prob = probs[index]\n",
        "        name = \"Unknown\"\n",
        "        if prob>0.5:\n",
        "            name = names[index]\n",
        "        putBoxText(image,x,y,w,h,text=name+\"({:.2f})\".format(prob))"
      ],
      "metadata": {
        "id": "Icqhk2_WNivb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im = cv2.imread('myself.jpg',1)\n",
        "putCharacters(im)\n",
        "plt.imshow(im)"
      ],
      "metadata": {
        "id": "F-3cmHknNiyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXv7-RMqNi07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NcyHcZRNi4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}